Unlike the the Simple Single Layer Perceptron (SLP) implemented above, a Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While in a SLP the neuron must have a linear activation function that imposes a threshold, neurons in a MLP can use any arbitrary (including non-linear) activation function. This conveys that a MLP can learn non-linear target functions, i.e. learn a non-linear relationship between the inputs and outputs. If a neural network consists of only linear activation functions (like a SLP), it can only model a linear relationship between the inputs and outputs, which makes it not as expressive or useful in a lot of applications.